{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>An Object Oriented Approach To Web-Scrapping</center>\n",
    "### Here we're scrapping friend's photos from facebook.\n",
    "### But there are some common methods/ classes which can be used to scrape any site with minor changes\n",
    "p.s., This project is purely developed for <b>learning purposes</b> and is intended to be used for same.\n",
    "<br>p.p.s., <b>Avoid using on google colab</b>, probably due to some ip restrictions/ proxies, you won't be able to scrape there. But yeah, you may try. Either you'll end up banging your head on the wall or enlighten us all with a solution! :P\n",
    "### Happy <strike>Scraping</strike> Learning! \n",
    "## Do refer to legal risks involved in scraping and do it at your own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a common class for all scraping functions\n",
    "### It has the below methods\n",
    "<ul>\n",
    "    <li> <b>Get Session</b>: Creates and returns a new session. Also adds User Agent header so that request doesn't look like coming from a bot</li>\n",
    "    <li> <b>Get Existing Session Cookies</b>: It checks if a session exists already on the basis of cookie provided and uses the same instead of getting new</li>\n",
    "    <li> <b>Save Cookie</b>:Saves a cookie at the path so that session can use this cookie instead of creating a new session everytime</li>\n",
    "    <li> <b>Delete Session:</b> Deletes the associated cookies and the session</li>\n",
    "    <li> <b>Make Request:</b> Makes a get/ post request. Returns <strike>yummy</strike> beautiful soup (:P) if asked for!</li>\n",
    "    <li> <b>Download Photos:</b> Makes a get request to get image stream and download at specified path</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilMethods:\n",
    "    \n",
    "    def get_session(self):\n",
    "        #Adding User Agent is very important otherwise end point can recognise it's a bot!! \n",
    "        headers = {  # This is the important part: Nokia C3 User Agent\n",
    "            'User-Agent': 'NokiaC3-00/5.0 (07.20) Profile/MIDP-2.1 Configuration/CLDC-1.1 Mozilla/5.0 AppleWebKit/420+ (KHTML, like Gecko) Safari/420+'\n",
    "        }\n",
    "        session = requests.session()  # Create the session for the next requests\n",
    "        session.headers.update(headers)\n",
    "        return session, headers\n",
    "    \n",
    "    # Evaluate if NOT exists a cookie file, if NOT exists the we make the Login request to Facebook,\n",
    "        # else we just load the current cookie to maintain the older session.\n",
    "    def get_existing_session_cookies(self, cookies_path):\n",
    "        if not os.path.exists(cookies_path):\n",
    "            return None\n",
    "        print(\"Session exists! No need to relogin!\")\n",
    "        f = open(cookies_path, 'rb')\n",
    "        \n",
    "        #Python object serialization\n",
    "        #“Pickling” is the process whereby a Python object hierarchy is converted into a byte stream\n",
    "        cookies = pickle.load(f)\n",
    "        return cookies\n",
    "    \n",
    "    def save_cookies(self, cookies_path, cookies):\n",
    "        f = open(cookies_path, 'wb')\n",
    "        pickle.dump(cookies, f)\n",
    "        \n",
    "    def delete_session(self, cookies_path, session):\n",
    "        if os.path.exists(cookies_path):\n",
    "            print(\"Session exists! Need to delete!\")\n",
    "            session.cookies.clear()\n",
    "            os.remove(cookies_path)\n",
    "            if not os.path.exists(cookies_path):\n",
    "                print(\"Cookie deleted successfully\")\n",
    "        else:\n",
    "            print(\"No cookie present\")\n",
    "    \n",
    "    # Utility function to make the requests and convert to soup object if necessary\n",
    "    def make_request(self, url, session, headers, method='GET', data=None, is_soup=True, stream = False):\n",
    "        if len(url) == 0:\n",
    "            raise Exception(f'Empty Url')\n",
    "        if method == 'GET':\n",
    "            resp = session.get(url, headers=headers, stream = stream)\n",
    "        elif method == 'POST':\n",
    "            resp = session.post(url, headers=headers, data=data)\n",
    "        else:\n",
    "            raise Exception(f'Method [{method}] Not Supported')\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f'Error [{resp.status_code}] > {url}')\n",
    "        \n",
    "        if is_soup:\n",
    "            return BeautifulSoup(resp.text, 'lxml')\n",
    "        return resp\n",
    "    \n",
    "    def download_photos(self, file_name, path, url, session, headers):\n",
    "        try:\n",
    "            newpath = r''+path \n",
    "            if not os.path.exists(newpath):\n",
    "                os.makedirs(newpath)\n",
    "            image_response = self.make_request(url, session, headers, stream = True, is_soup = False)\n",
    "            with open(newpath+'/'+file_name+\"_img.jpg\", \"wb\") as out_file:\n",
    "                shutil.copyfileobj(image_response.raw, out_file)\n",
    "                print('File saved successfully!! Cheers! :D')\n",
    "        except KeyError:\n",
    "            #response is an empty dictionary\n",
    "            print('Uh Uh! Please check the url/ user id/ your network connection and try again later')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class for logging into the site to scrape\n",
    "### It has the below methods\n",
    "<ul>\n",
    "    <li> <b>Constructor</b>: Takes username and password (But never saves them! Makes a call to login and deletes them!) Constructor is also responsible for getting a new or an existing session if cookie is already present.</li>\n",
    "    <li> <b>Get Login Form Data</b>: Creates form data to login</li>\n",
    "    <li> <b>Logout</b>: Makes a call to utility function clear cookies to delete the session.</li>\n",
    "    <li> Also has a class variable, <b>Util</b> to have an instance for <b>Utility Methods</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoginToFacebook:\n",
    "    session = None\n",
    "    base_url = \"https://m.facebook.com\"\n",
    "    util = UtilMethods()\n",
    "    def __init__(self, username, password, create_new_session=False):\n",
    "        \n",
    "        self.session, self.headers = self.util.get_session()\n",
    "        self.cookies_path = 'session_facebook.cki'  # Give a name to store the session in a cookie file.\n",
    "        if create_new_session:\n",
    "            self.login(username, password)\n",
    "        else:\n",
    "            cookies = self.util.get_existing_session_cookies(self.cookies_path)\n",
    "            if cookies is None:\n",
    "                self.login(username, password)\n",
    "            else:\n",
    "                self.session.cookies = cookies\n",
    "        \n",
    "        ''' We should not be saving sensitive information like password in class variable whether public or private.\n",
    "        Hence use it to login, get session and delete it!'''\n",
    "        del username\n",
    "        del password\n",
    "        \n",
    "        # At certain point, we need find the text in the Url to point the url post, in my case, my Facebook is in\n",
    "        # English, this is why it says 'Full Story', so, you need to change this for your language.\n",
    "        # Some translations:\n",
    "        # - English: 'Full Story'\n",
    "        self.post_url_text = 'Full Story'\n",
    "        self.posts = []  # Store the scraped posts\n",
    "\n",
    "    # The first time we login\n",
    "    def login(self, username, password):\n",
    "        print(\"Session doesn't exist! Relogging to facebook!\")\n",
    "        # Get the content of HTML of mobile Login Facebook page\n",
    "        soup = self.util.make_request(self.base_url, self.session, self.headers)\n",
    "        if soup is None:\n",
    "            raise Exception(\"Couldn't load the Login Page\")\n",
    "        \n",
    "        # This is the url to send the login params to Facebook\n",
    "        url_login = \"https://m.facebook.com/login/device-based/regular/login/?refsrc=https%3A%2F%2Fm.facebook.com%2F&lwv=100&refid=8\"\n",
    "        payload = self.get_login_form_data(soup, username, password)\n",
    "        soup = self.util.make_request(url_login, self.session, self.headers, method='POST', data=payload, is_soup=True)\n",
    "        if soup is None:\n",
    "            raise Exception(f\"The login request couldn't be made: {url_login}\")\n",
    "        \n",
    "        redirect = soup.select_one('a') #First anchor tag is user for redirecting\n",
    "        if not redirect:\n",
    "            raise Exception(\"Please log in desktop/mobile Facebook and change your password\")\n",
    "        url_redirect = redirect.get('href', '')\n",
    "        resp = self.util.make_request(url_redirect, self.session, self.headers)\n",
    "        if resp is None:\n",
    "            raise Exception(f\"The login request couldn't be made: {url_redirect}\")\n",
    "\n",
    "        # Finally we get the cookies from the session and save it in a file for future usage\n",
    "        self.util.save_cookies(self.cookies_path, self.session.cookies)\n",
    "        print('Logged in successfully')\n",
    "    \n",
    "    #function to prepare login form data\n",
    "    def get_login_form_data(self, soup, username, password):\n",
    "        '''Here we need to extract this tokens from the Login Page\n",
    "        These are the values used for login form. \n",
    "        If we do not add these with credentials, it would be detected as suspicious activity \n",
    "        and account might get blocked too!'''\n",
    "        \n",
    "        #You can find these values by browsing the website and opening inspect element and look for corresponding tags\n",
    "        lsd = soup.find(\"input\", {\"name\": \"lsd\"}).get(\"value\")\n",
    "        jazoest = soup.find(\"input\", {\"name\": \"jazoest\"}).get(\"value\")\n",
    "        m_ts = soup.find(\"input\", {\"name\": \"m_ts\"}).get(\"value\")\n",
    "        li = soup.find(\"input\", {\"name\": \"li\"}).get(\"value\")\n",
    "        try_number = soup.find(\"input\", {\"name\": \"try_number\"}).get(\"value\")\n",
    "        unrecognized_tries = soup.find(\"input\", {\"name\": \"unrecognized_tries\"}).get(\"value\")\n",
    "        payload = {\n",
    "            \"lsd\": lsd,\n",
    "            \"jazoest\": jazoest,\n",
    "            \"m_ts\": m_ts,\n",
    "            \"li\": li,\n",
    "            \"try_number\": try_number,\n",
    "            \"unrecognized_tries\": unrecognized_tries,\n",
    "            \"email\": username,\n",
    "            \"pass\": password,\n",
    "            \"login\": \"start session\",\n",
    "            \"prefill_contact_point\": \"\",\n",
    "            \"prefill_source\": \"\",\n",
    "            \"prefill_type\": \"\",\n",
    "            \"first_prefill_source\": \"\",\n",
    "            \"first_prefill_type\": \"\",\n",
    "            \"had_cp_prefilled\": \"false\",\n",
    "            \"had_password_prefilled\": \"false\",\n",
    "            \"is_smart_lock\": \"false\",\n",
    "            \"_fb_noscript\": \"true\"\n",
    "        }\n",
    "        return payload\n",
    "    \n",
    "    def logout(self):\n",
    "        self.util.delete_session(self.cookies_path, self.session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class for getting the profile to scrape\n",
    "### A Child class for Login\n",
    "### It has the below methods\n",
    "<ul>\n",
    "    <li> <b>Constructor</b>: Takes username and password and passes it to super class.</li>\n",
    "    <li> <b>Add Base Url</b>: A method to add base site url</li>\n",
    "    <li> <b>Create Profile Url from Scratch</b>:If this class is being provided a user name instead of url, this method generates url</li>\n",
    "    <li> <b>Get Username From Url:</b> If this class is being provided a url instead of user name, this method generates username</li>\n",
    "    <li> <b>Prepare Profile Url:</b> Appends basic details to profile url or creates a profile from scratch if username is given</li>\n",
    "    <li> <b>Get Profile:</b> Gets the requested profile's beautiful soup internally calling other methods to generate the same</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileToScrape(LoginToFacebook):\n",
    "    \n",
    "    def __init__(self, username, password, create_new_session = False):\n",
    "        super().__init__(username, password, create_new_session)\n",
    "    \n",
    "    def add_base_url(self, data):\n",
    "        url_suffix = str(data)\n",
    "        if url_suffix.startswith('/'):\n",
    "            return self.base_url + url_suffix\n",
    "        else:\n",
    "            return self.base_url + '/' + url_suffix\n",
    "        \n",
    "    def create_profile_url_from_scratch(self, user_id):\n",
    "        # Prepare the Url to point to the posts feed\n",
    "        return self.add_base_url(user_id) + '?v=timeline'\n",
    "    \n",
    "    def get_username_from_url(self, url_profile):\n",
    "        username = None\n",
    "        if url_profile[-1] == '/' or url_profile[-1] == '?':\n",
    "            url_profile = url_profile[:-1]\n",
    "        username = re.findall('com/(.+)', url_profile)\n",
    "        return username\n",
    "    \n",
    "    def prepare_profile_url(self, url_profile):\n",
    "        # Prepare the Url to point to the posts feed\n",
    "        if \"www.\" in url_profile: url_profile = url_profile.replace('www.', 'm.')\n",
    "        if 'v=timeline' not in url_profile:\n",
    "            if '?' in url_profile:\n",
    "                url_profile = f'{url_profile}&v=timeline'\n",
    "            else:\n",
    "                url_profile = f'{url_profile}?v=timeline'\n",
    "        return url_profile\n",
    "    \n",
    "    def get_profile(self, user_id, is_url = False):\n",
    "        usr_profile = ''\n",
    "        soup = None\n",
    "        is_profile_a_group = False\n",
    "        username = None\n",
    "        if is_url:\n",
    "            username = self.get_username_from_url(user_id)\n",
    "            if username is None or len(username) == 0:\n",
    "                raise Exception(f\"User not found: {user_id}\")\n",
    "            else:\n",
    "                username = username[0]\n",
    "            usr_profile = self.prepare_profile_url(user_id)\n",
    "            is_profile_a_group = '/groups/' in usr_profile\n",
    "        else:\n",
    "            usr_profile = self.create_profile_url_from_scratch(user_id)\n",
    "            username = user_id\n",
    "            \n",
    "        # Make a simple GET request\n",
    "        return self.util.make_request(usr_profile, self.session, self.headers), is_profile_a_group, username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class for taking the scraping actions on profile\n",
    "### A Child class for Profile\n",
    "### It has the below methods\n",
    "<ul>\n",
    "    <li> <b>Constructor</b>: Takes username and password and passes it to super class.</li>\n",
    "    <li> <b>Download Multiple Users Photos</b>: Takes a list of user ids/ user urls to scrape their photos, which iterative calls method for scraping single profile</li>\n",
    "    <li> <b>Download User Photos</b>:Gets the profile from parent class and uses beautiful soup to generate set(for uniqueness) of urls of the user photos</li>\n",
    "    <li> <b>Get Image Url From Soup:</b> It is used to get image urls from soup which is further giving to utility methods download images method to save it to your local system</li>\n",
    "    <li>Hey wait, here's a bonus parameter too!</li>\n",
    "    <li><b>Restricting number of images per user:</b> Since social network sites are full of images, we can restrict images per user.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeFromFacebook(ProfileToScrape):\n",
    "    \n",
    "    def __init__(self, username, password, create_new_session = False):\n",
    "        super().__init__(username, password, create_new_session)\n",
    "    \n",
    "    def download_multiple_users_photos(self, url_profiles, is_url = False, max_len=None):\n",
    "        for url_profile in url_profiles:\n",
    "            self.download_user_photos(url_profile, is_url = is_url, max_len = max_len)\n",
    "        \n",
    "    def download_user_photos(self, url_profile, is_url = False, max_len = None):\n",
    "        soup, is_group, username = self.get_profile(url_profile, is_url)\n",
    "        list_photo_urls = set()\n",
    "        if soup is None:\n",
    "            print(f\"Couldn't load the Page: {url_profile}\")\n",
    "            return []\n",
    "        \n",
    "        photos_url =(soup.find('a',{'href': re.compile(r'\\bphotos\\?lst\\b')}))['href']\n",
    "        \n",
    "        if photos_url is not None:\n",
    "            photo_soup = self.util.make_request(self.add_base_url(photos_url), self.session, self.headers, is_soup=True)\n",
    "            \n",
    "            # contains link to tagged photos and user's own photos\n",
    "            albums_link = photo_soup.findAll('a',text='See All')\n",
    "            if len(albums_link)>0:\n",
    "                for i in range(len(albums_link)):\n",
    "                    soup = self.util.make_request(self.add_base_url((albums_link[i])['href']), self.session, self.headers)\n",
    "                    list_photo_urls.update([self.add_base_url((photo_id)['href']) for photo_id in soup.findAll('a',{'href': re.compile(r'\\bphoto\\.php\\?fbid\\b')})])\n",
    "                    has_see_more = soup.findAll('span',text='See more photos')\n",
    "                    while has_see_more is not None and len(has_see_more) > 0:\n",
    "                        a_href = ((has_see_more[0]).find('a'))['href']\n",
    "                        soup = self.util.make_request(self.add_base_url(a_href), self.session, self.headers, is_soup=True)\n",
    "                        list_photo_urls.update([self.add_base_url((photo_id)['href']) for photo_id in soup.findAll('a',{'href': re.compile(r'\\bphoto\\.php\\?fbid\\b')})])\n",
    "                        has_see_more = soup.findAll('span',text='See more photos')    \n",
    "                \n",
    "            else:\n",
    "                #no 'See All', probably user has just 1 photo\n",
    "                list_photo_urls.update([self.add_base_url((photo_id)['href']) for photo_id in photo_soup.findAll('a',{'href': re.compile(r'\\bphoto\\.php\\?fbid\\b')})])\n",
    "        \n",
    "        if len(list_photo_urls) == 0:\n",
    "            print('No photos available for '+username)\n",
    "        else:\n",
    "            print('Total number of photos for '+username,len(list_photo_urls))\n",
    "            self.get_image_url_from_soup(list_photo_urls, username, max_len)\n",
    "    \n",
    "    def get_image_url_from_soup(self, urls, username, max_len = None):\n",
    "        for idx, url in enumerate(urls):\n",
    "            if max_len is not None and idx == max_len:\n",
    "                break\n",
    "            soup = self.util.make_request(url, self.session, self.headers)\n",
    "            root_div = soup.find(\"div\", {\"id\": \"root\"})\n",
    "            img = root_div.find(\"img\")\n",
    "            self.util.download_photos('img'+str(idx), username, img['src'], self.session, self.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session doesn't exist! Relogging to facebook!\n",
      "Logged in successfully\n"
     ]
    }
   ],
   "source": [
    "fb = ScrapeFromFacebook('username', 'password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of photos for rushil.sehgal 28\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n"
     ]
    }
   ],
   "source": [
    "#downloading photos for single profile\n",
    "fb.download_user_photos('heymsakshi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of photos for neetukalwan 261\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "Total number of photos for rushil.sehgal 28\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n"
     ]
    }
   ],
   "source": [
    "#downloading photos for multiple urls\n",
    "fb.download_multiple_users_photos(['https://www.facebook.com/neetukalwan','https://www.facebook.com/rushil.sehgal'], is_url=True, max_len=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of photos for User 28\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n",
      "File saved successfully!! Cheers! :D\n"
     ]
    }
   ],
   "source": [
    "a = fb.download_user_photos('rushil.sehgal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session exists! Need to delete!\n",
      "Cookie deleted successfully\n"
     ]
    }
   ],
   "source": [
    "#Don't forget to logout when done so that you end up giving unwanted access of your profile to someone else\n",
    "fb.logout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
